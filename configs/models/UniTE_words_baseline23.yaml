unite_metric_multi_task:
  class_path: comet.models.UniTEMetricMT
  init_args:
    nr_frozen_epochs: 0.3
    keep_embeddings_frozen: True
    optimizer: AdamW
    encoder_learning_rate: 1.0e-06
    learning_rate: 1.0e-05
    layerwise_decay: 0.95
    encoder_model: XLM-RoBERTa
    pretrained_model: microsoft/infoxlm-large
    pool: cls
    loss: mse
    dropout: 0.1
    batch_size: 4
    train_data: 
      - data/word-level-subtask/hter/train-dev.csv
    sampling_gamma: 1.0
    validation_data: 
      - data/word-level-subtask/da/dev/en-cs-dev.csv
      - data/word-level-subtask/da/dev/en-de-dev.csv
      - data/word-level-subtask/da/dev/en-ja-dev.csv
      - data/word-level-subtask/da/dev/en-zh-dev.csv
      - data/word-level-subtask/da/dev/et-en-dev.csv
      - data/word-level-subtask/da/dev/et-en-dev.csv
      - data/word-level-subtask/da/dev/ne-en-dev.csv
      - data/word-level-subtask/da/dev/ro-en-dev.csv
      - data/word-level-subtask/da/dev/ru-en-dev.csv
      - data/word-level-subtask/da/dev/si-en-dev.csv
      - data/word-level-subtask/da/dev/km-en-dev.csv
      - data/word-level-subtask/da/dev/ps-en-dev.csv

    hidden_sizes:
      - 2048
      - 1024
    activations: Tanh
    rnn_hidden_size: 1024
    rnn_hidden_layers: 2
    word_weights:
      - 0.3
      - 0.7
    input_segments:
      - mt
      - src
    unite_training: false
    use_rnn: false
    qe_training: True
    word_layer: 24
    word_level_training: true
    

    
trainer: ../trainer.yaml
early_stopping: ../early_stopping.yaml
model_checkpoint: ../model_checkpoint.yaml
